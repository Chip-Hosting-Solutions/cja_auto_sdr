{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:51:20,877 - INFO - Logging initialized. Log file: logs/SDR_Generation_dv_677ea9291244fd082f02dd42_20260105_185120.log\n",
      "2026-01-05 18:51:20,878 - INFO - ============================================================\n",
      "2026-01-05 18:51:20,878 - INFO - INITIALIZING CJA CONNECTION\n",
      "2026-01-05 18:51:20,879 - INFO - ============================================================\n",
      "2026-01-05 18:51:20,879 - INFO - Validating configuration file: myconfig.json\n",
      "2026-01-05 18:51:20,880 - WARNING - Configuration file may be missing required fields: tech_id, private_key\n",
      "2026-01-05 18:51:20,880 - WARNING - This may cause authentication failures\n",
      "2026-01-05 18:51:20,880 - INFO - Loading CJA configuration...\n",
      "2026-01-05 18:51:20,881 - INFO - Configuration loaded successfully\n",
      "2026-01-05 18:51:20,881 - INFO - Creating CJA instance...\n",
      "2026-01-05 18:51:21,037 - INFO - CJA instance created successfully\n",
      "2026-01-05 18:51:21,037 - INFO - Testing API connection...\n",
      "2026-01-05 18:51:21,473 - INFO - ✓ API connection successful! Found 85 data view(s)\n",
      "2026-01-05 18:51:21,473 - INFO - CJA initialization complete\n",
      "2026-01-05 18:51:21,474 - INFO - ✓ CJA connection established successfully\n",
      "2026-01-05 18:51:21,474 - INFO - ============================================================\n",
      "2026-01-05 18:51:21,475 - INFO - VALIDATING DATA VIEW\n",
      "2026-01-05 18:51:21,475 - INFO - ============================================================\n",
      "2026-01-05 18:51:21,475 - INFO - Data View ID: dv_677ea9291244fd082f02dd42\n",
      "2026-01-05 18:51:21,476 - INFO - Fetching data view information from API...\n",
      "2026-01-05 18:51:21,797 - INFO - ✓ Data view validated successfully!\n",
      "2026-01-05 18:51:21,797 - INFO -   Name: Adobe Store - Prod (CJA Lab L123 Adobe Summit 2025)\n",
      "2026-01-05 18:51:21,798 - INFO -   ID: dv_677ea9291244fd082f02dd42\n",
      "2026-01-05 18:51:21,798 - INFO -   Owner: Brian Au\n",
      "2026-01-05 18:51:21,798 - INFO - ✓ Data view validation complete - proceeding with data fetch\n",
      "2026-01-05 18:51:21,799 - INFO - ============================================================\n",
      "2026-01-05 18:51:21,799 - INFO - Starting data fetch operations\n",
      "2026-01-05 18:51:21,800 - INFO - ============================================================\n",
      "2026-01-05 18:51:21,800 - INFO - Fetching metrics for data view: dv_677ea9291244fd082f02dd42\n",
      "2026-01-05 18:51:22,386 - INFO - Successfully fetched 30 metrics\n",
      "2026-01-05 18:51:22,386 - INFO - Fetching dimensions for data view: dv_677ea9291244fd082f02dd42\n",
      "2026-01-05 18:51:22,902 - INFO - Successfully fetched 104 dimensions\n",
      "2026-01-05 18:51:22,903 - INFO - Fetching data view information: dv_677ea9291244fd082f02dd42\n",
      "2026-01-05 18:51:23,238 - INFO - Successfully fetched data view info: Adobe Store - Prod (CJA Lab L123 Adobe Summit 2025)\n",
      "2026-01-05 18:51:23,239 - INFO - Data fetch operations completed successfully\n",
      "2026-01-05 18:51:23,240 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,240 - INFO - Starting data quality validation\n",
      "2026-01-05 18:51:23,241 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,241 - INFO - Running comprehensive data quality checks...\n",
      "2026-01-05 18:51:23,248 - WARNING - DQ Issue [MEDIUM] - Metrics: Null values in \"description\" field\n",
      "2026-01-05 18:51:23,248 - WARNING - DQ Issue [MEDIUM] - Dimensions: Null values in \"description\" field\n",
      "2026-01-05 18:51:23,249 - WARNING - DQ Issue [LOW] - Metrics: 12 items without descriptions\n",
      "2026-01-05 18:51:23,249 - WARNING - DQ Issue [LOW] - Dimensions: 64 items without descriptions\n",
      "2026-01-05 18:51:23,250 - INFO - Data quality checks complete. Found 4 issue(s)\n",
      "2026-01-05 18:51:23,252 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,253 - INFO - Processing data for Excel export\n",
      "2026-01-05 18:51:23,253 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,254 - INFO - Processing data view lookup information...\n",
      "2026-01-05 18:51:23,254 - INFO - Processed lookup data with 1 rows\n",
      "2026-01-05 18:51:23,255 - INFO - Creating metadata summary...\n",
      "2026-01-05 18:51:23,256 - INFO - Metadata created successfully\n",
      "2026-01-05 18:51:23,256 - INFO - Applying JSON formatting to dataframes...\n",
      "2026-01-05 18:51:23,265 - INFO - JSON formatting applied successfully\n",
      "2026-01-05 18:51:23,266 - INFO - Excel file will be saved as: CJA_DataView_Adobe Store - Prod CJA Lab L123 Adobe Summit 2025_dv_677ea9291244fd082f02dd42_SDR.xlsx\n",
      "2026-01-05 18:51:23,266 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,267 - INFO - Generating Excel file\n",
      "2026-01-05 18:51:23,267 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,268 - INFO - Creating Excel writer for: CJA_DataView_Adobe Store - Prod CJA Lab L123 Adobe Summit 2025_dv_677ea9291244fd082f02dd42_SDR.xlsx\n",
      "2026-01-05 18:51:23,297 - INFO - Formatting sheet: Metadata\n",
      "2026-01-05 18:51:23,299 - INFO - Successfully formatted sheet: Metadata\n",
      "2026-01-05 18:51:23,300 - INFO - Formatting sheet: Data Quality\n",
      "2026-01-05 18:51:23,301 - INFO - Successfully formatted sheet: Data Quality\n",
      "2026-01-05 18:51:23,301 - INFO - Formatting sheet: DataView\n",
      "2026-01-05 18:51:23,302 - INFO - Successfully formatted sheet: DataView\n",
      "2026-01-05 18:51:23,303 - INFO - Formatting sheet: Metrics\n",
      "2026-01-05 18:51:23,308 - INFO - Successfully formatted sheet: Metrics\n",
      "2026-01-05 18:51:23,308 - INFO - Formatting sheet: Dimensions\n",
      "2026-01-05 18:51:23,324 - INFO - Successfully formatted sheet: Dimensions\n",
      "2026-01-05 18:51:23,358 - INFO - ✓ SDR generation complete! File saved as: CJA_DataView_Adobe Store - Prod CJA Lab L123 Adobe Summit 2025_dv_677ea9291244fd082f02dd42_SDR.xlsx\n",
      "2026-01-05 18:51:23,359 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,359 - INFO - EXECUTION SUMMARY\n",
      "2026-01-05 18:51:23,359 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,359 - INFO - Data View: Adobe Store - Prod CJA Lab L123 Adobe Summit 2025 (dv_677ea9291244fd082f02dd42)\n",
      "2026-01-05 18:51:23,360 - INFO - Metrics: 30\n",
      "2026-01-05 18:51:23,360 - INFO - Dimensions: 104\n",
      "2026-01-05 18:51:23,360 - INFO - Data Quality Issues: 4\n",
      "2026-01-05 18:51:23,361 - INFO - Data Quality Issues by Severity:\n",
      "2026-01-05 18:51:23,361 - INFO -   MEDIUM: 2\n",
      "2026-01-05 18:51:23,361 - INFO -   LOW: 2\n",
      "2026-01-05 18:51:23,362 - INFO - Output file: CJA_DataView_Adobe Store - Prod CJA Lab L123 Adobe Summit 2025_dv_677ea9291244fd082f02dd42_SDR.xlsx\n",
      "2026-01-05 18:51:23,362 - INFO - ============================================================\n",
      "2026-01-05 18:51:23,362 - INFO - Script execution completed successfully\n"
     ]
    }
   ],
   "source": [
    "import cjapy\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# ==================== LOGGING SETUP ====================\n",
    "\n",
    "def setup_logging(data_view_id: str) -> logging.Logger:\n",
    "    \"\"\"Setup logging to both file and console\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = Path(\"logs\")\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create log filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = log_dir / f\"SDR_Generation_{data_view_id}_{timestamp}.log\"\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Logging initialized. Log file: {log_file}\")\n",
    "    return logger\n",
    "\n",
    "# Set the Data View id we want into a variable\n",
    "data_view = \"dv_677ea9291244fd082f02dd42\"\n",
    "\n",
    "# Initialize logging\n",
    "logger = setup_logging(data_view)\n",
    "\n",
    "# ==================== CJA INITIALIZATION ====================\n",
    "\n",
    "def validate_config_file(config_file: str) -> bool:\n",
    "    \"\"\"Validate configuration file exists and has required structure\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Validating configuration file: {config_file}\")\n",
    "        \n",
    "        config_path = Path(config_file)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not config_path.exists():\n",
    "            logger.error(f\"Configuration file not found: {config_path.absolute()}\")\n",
    "            logger.error(f\"Please ensure '{config_file}' exists in the current directory\")\n",
    "            return False\n",
    "        \n",
    "        # Check if file is readable\n",
    "        if not config_path.is_file():\n",
    "            logger.error(f\"'{config_file}' is not a valid file\")\n",
    "            return False\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "            \n",
    "            # Check for required fields in config\n",
    "            required_fields = ['org_id', 'client_id', 'tech_id', 'secret', 'private_key']\n",
    "            missing_fields = [field for field in required_fields if field not in config_data]\n",
    "            \n",
    "            if missing_fields:\n",
    "                logger.warning(f\"Configuration file may be missing required fields: {', '.join(missing_fields)}\")\n",
    "                logger.warning(\"This may cause authentication failures\")\n",
    "            else:\n",
    "                logger.info(\"Configuration file structure validated successfully\")\n",
    "            \n",
    "            # Check for empty values\n",
    "            empty_fields = [field for field in required_fields if field in config_data and not config_data[field]]\n",
    "            if empty_fields:\n",
    "                logger.warning(f\"Configuration file has empty values for: {', '.join(empty_fields)}\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Configuration file is not valid JSON: {str(e)}\")\n",
    "            logger.error(\"Please check the file format and ensure it's properly formatted JSON\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not fully validate config structure: {str(e)}\")\n",
    "            logger.info(\"Proceeding with initialization attempt...\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error validating config file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def initialize_cja(config_file: str = \"myconfig.json\") -> Optional[cjapy.CJA]:\n",
    "    \"\"\"Initialize CJA connection with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"INITIALIZING CJA CONNECTION\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Validate config file first\n",
    "        if not validate_config_file(config_file):\n",
    "            logger.critical(\"Configuration file validation failed\")\n",
    "            logger.critical(\"Please create a valid config file with the following structure:\")\n",
    "            logger.critical(json.dumps({\n",
    "                \"org_id\": \"your_org_id\",\n",
    "                \"client_id\": \"your_client_id\", \n",
    "                \"tech_id\": \"your_tech_account_id\",\n",
    "                \"secret\": \"your_client_secret\",\n",
    "                \"private_key\": \"path/to/private.key\"\n",
    "            }, indent=2))\n",
    "            return None\n",
    "        \n",
    "        # Attempt to import config\n",
    "        logger.info(\"Loading CJA configuration...\")\n",
    "        cjapy.importConfigFile(config_file)\n",
    "        logger.info(\"Configuration loaded successfully\")\n",
    "        \n",
    "        # Attempt to create CJA instance\n",
    "        logger.info(\"Creating CJA instance...\")\n",
    "        cja = cjapy.CJA()\n",
    "        logger.info(\"CJA instance created successfully\")\n",
    "        \n",
    "        # Test connection with a simple API call\n",
    "        logger.info(\"Testing API connection...\")\n",
    "        try:\n",
    "            # Attempt to list data views to verify connection\n",
    "            test_call = cja.getDataViews()\n",
    "            if test_call is not None:\n",
    "                logger.info(f\"✓ API connection successful! Found {len(test_call) if hasattr(test_call, '__len__') else 'multiple'} data view(s)\")\n",
    "            else:\n",
    "                logger.warning(\"API connection test returned None - connection may be unstable\")\n",
    "        except Exception as test_error:\n",
    "            logger.warning(f\"Could not verify connection with test call: {str(test_error)}\")\n",
    "            logger.warning(\"Proceeding anyway - errors may occur during data fetching\")\n",
    "        \n",
    "        logger.info(\"CJA initialization complete\")\n",
    "        return cja\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(\"CONFIGURATION FILE ERROR\")\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(f\"Config file not found: {config_file}\")\n",
    "        logger.critical(f\"Current working directory: {Path.cwd()}\")\n",
    "        logger.critical(\"Please ensure the configuration file exists in the correct location\")\n",
    "        return None\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(\"DEPENDENCY ERROR\")\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(f\"Failed to import cjapy module: {str(e)}\")\n",
    "        logger.critical(\"Please ensure cjapy is installed: pip install cjapy\")\n",
    "        return None\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(\"CJA CONFIGURATION ERROR\")\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(f\"Configuration error: {str(e)}\")\n",
    "        logger.critical(\"This usually indicates an issue with the authentication credentials\")\n",
    "        logger.critical(\"Please verify all fields in your configuration file are correct\")\n",
    "        return None\n",
    "        \n",
    "    except PermissionError as e:\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(\"PERMISSION ERROR\")\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(f\"Cannot read configuration file: {str(e)}\")\n",
    "        logger.critical(\"Please check file permissions\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(\"CJA INITIALIZATION FAILED\")\n",
    "        logger.critical(\"=\" * 60)\n",
    "        logger.critical(f\"Unexpected error: {str(e)}\")\n",
    "        logger.critical(f\"Error type: {type(e).__name__}\")\n",
    "        logger.exception(\"Full error details:\")\n",
    "        logger.critical(\"\")\n",
    "        logger.critical(\"Troubleshooting steps:\")\n",
    "        logger.critical(\"1. Verify your configuration file exists and is valid JSON\")\n",
    "        logger.critical(\"2. Check that all authentication credentials are correct\")\n",
    "        logger.critical(\"3. Ensure your API credentials have the necessary permissions\")\n",
    "        logger.critical(\"4. Verify you have network connectivity to Adobe services\")\n",
    "        logger.critical(\"5. Check if cjapy library is up to date: pip install --upgrade cjapy\")\n",
    "        return None\n",
    "\n",
    "# Initialize CJA with comprehensive error handling\n",
    "cja = initialize_cja()\n",
    "\n",
    "if cja is None:\n",
    "    logger.critical(\"=\" * 60)\n",
    "    logger.critical(\"FATAL ERROR: Cannot proceed without CJA connection\")\n",
    "    logger.critical(\"=\" * 60)\n",
    "    logger.critical(\"Script execution terminated\")\n",
    "    logger.critical(f\"Please check the log file for details: {Path('logs').absolute()}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(\"✓ CJA connection established successfully\")\n",
    "\n",
    "# ==================== DATA VIEW VALIDATION ====================\n",
    "\n",
    "def validate_data_view(cja: cjapy.CJA, data_view_id: str) -> bool:\n",
    "    \"\"\"Validate that the data view exists and is accessible with detailed error reporting\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"VALIDATING DATA VIEW\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Data View ID: {data_view_id}\")\n",
    "        \n",
    "        # Basic format validation\n",
    "        if not data_view_id or not isinstance(data_view_id, str):\n",
    "            logger.error(\"Invalid data view ID format\")\n",
    "            logger.error(\"Data view ID must be a non-empty string\")\n",
    "            return False\n",
    "        \n",
    "        if not data_view_id.startswith('dv_'):\n",
    "            logger.warning(f\"Data view ID '{data_view_id}' does not follow standard format (dv_...)\")\n",
    "            logger.warning(\"This may still be valid, but unusual\")\n",
    "        \n",
    "        # Attempt to fetch data view info\n",
    "        logger.info(\"Fetching data view information from API...\")\n",
    "        try:\n",
    "            dv_info = cja.getDataView(data_view_id)\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"API method 'getDataView' not available\")\n",
    "            logger.error(\"This may indicate an outdated version of cjapy\")\n",
    "            logger.error(\"Please update cjapy: pip install --upgrade cjapy\")\n",
    "            return False\n",
    "        except Exception as api_error:\n",
    "            logger.error(f\"API call failed: {str(api_error)}\")\n",
    "            logger.error(\"Possible reasons:\")\n",
    "            logger.error(\"  1. Data view does not exist\")\n",
    "            logger.error(\"  2. You don't have permission to access this data view\")\n",
    "            logger.error(\"  3. Network connectivity issues\")\n",
    "            logger.error(\"  4. API authentication has expired\")\n",
    "            return False\n",
    "        \n",
    "        # Validate response\n",
    "        if not dv_info:\n",
    "            logger.error(f\"Data view '{data_view_id}' returned empty response\")\n",
    "            logger.error(\"This typically means:\")\n",
    "            logger.error(\"  - The data view does not exist\")\n",
    "            logger.error(\"  - You don't have access to this data view\")\n",
    "            logger.error(\"  - The data view ID is incorrect\")\n",
    "            \n",
    "            # Try to list available data views to help user\n",
    "            logger.info(\"Attempting to list available data views...\")\n",
    "            try:\n",
    "                available_dvs = cja.getDataViews()\n",
    "                if available_dvs and len(available_dvs) > 0:\n",
    "                    logger.info(f\"You have access to {len(available_dvs)} data view(s):\")\n",
    "                    for i, dv in enumerate(available_dvs[:10]):  # Show first 10\n",
    "                        dv_id = dv.get('id', 'unknown')\n",
    "                        dv_name = dv.get('name', 'unknown')\n",
    "                        logger.info(f\"  {i+1}. {dv_name} (ID: {dv_id})\")\n",
    "                    if len(available_dvs) > 10:\n",
    "                        logger.info(f\"  ... and {len(available_dvs) - 10} more\")\n",
    "                else:\n",
    "                    logger.warning(\"No data views found - you may not have access to any data views\")\n",
    "            except Exception as list_error:\n",
    "                logger.warning(f\"Could not list available data views: {str(list_error)}\")\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        # Extract and validate data view details\n",
    "        dv_name = dv_info.get('name', 'Unknown')\n",
    "        dv_description = dv_info.get('description', 'No description')\n",
    "        dv_owner = dv_info.get('owner', {}).get('name', 'Unknown')\n",
    "        \n",
    "        logger.info(\"✓ Data view validated successfully!\")\n",
    "        logger.info(f\"  Name: {dv_name}\")\n",
    "        logger.info(f\"  ID: {data_view_id}\")\n",
    "        logger.info(f\"  Owner: {dv_owner}\")\n",
    "        if dv_description and dv_description != 'No description':\n",
    "            logger.info(f\"  Description: {dv_description[:100]}{'...' if len(dv_description) > 100 else ''}\")\n",
    "        \n",
    "        # Additional validation checks\n",
    "        warnings = []\n",
    "        \n",
    "        if 'components' in dv_info:\n",
    "            components = dv_info.get('components', {})\n",
    "            if not components.get('dimensions') and not components.get('metrics'):\n",
    "                warnings.append(\"Data view appears to have no components defined\")\n",
    "        \n",
    "        if warnings:\n",
    "            logger.warning(\"Data view validation warnings:\")\n",
    "            for warning in warnings:\n",
    "                logger.warning(f\"  - {warning}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"=\" * 60)\n",
    "        logger.error(\"DATA VIEW VALIDATION ERROR\")\n",
    "        logger.error(\"=\" * 60)\n",
    "        logger.error(f\"Unexpected error during validation: {str(e)}\")\n",
    "        logger.exception(\"Full error details:\")\n",
    "        logger.error(\"\")\n",
    "        logger.error(\"Please verify:\")\n",
    "        logger.error(\"  1. The data view ID is correct\")\n",
    "        logger.error(\"  2. You have access to this data view\")\n",
    "        logger.error(\"  3. Your API credentials are valid\")\n",
    "        return False\n",
    "\n",
    "# Validate data view before proceeding\n",
    "if not validate_data_view(cja, data_view):\n",
    "    logger.critical(\"=\" * 60)\n",
    "    logger.critical(\"FATAL ERROR: Data view validation failed\")\n",
    "    logger.critical(\"=\" * 60)\n",
    "    logger.critical(f\"Cannot proceed with invalid data view: {data_view}\")\n",
    "    logger.critical(\"\")\n",
    "    logger.critical(\"Please check:\")\n",
    "    logger.critical(\"  1. Verify the data view ID is correct\")\n",
    "    logger.critical(\"  2. Ensure you have permission to access this data view\")\n",
    "    logger.critical(\"  3. Confirm the data view exists in your organization\")\n",
    "    logger.critical(\"\")\n",
    "    logger.critical(\"Script execution terminated\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(\"✓ Data view validation complete - proceeding with data fetch\")\n",
    "\n",
    "# ==================== API DATA FETCHING ====================\n",
    "\n",
    "def fetch_metrics(cja: cjapy.CJA, data_view_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch metrics with error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Fetching metrics for data view: {data_view_id}\")\n",
    "        metrics = cja.getMetrics(data_view_id, inclType=True, full=True)\n",
    "        \n",
    "        if metrics is None or (isinstance(metrics, pd.DataFrame) and metrics.empty):\n",
    "            logger.warning(\"No metrics returned from API\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        logger.info(f\"Successfully fetched {len(metrics)} metrics\")\n",
    "        return metrics\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"API method error - getMetrics may not be available: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch metrics: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_dimensions(cja: cjapy.CJA, data_view_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch dimensions with error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Fetching dimensions for data view: {data_view_id}\")\n",
    "        dimensions = cja.getDimensions(data_view_id, inclType=True, full=True)\n",
    "        \n",
    "        if dimensions is None or (isinstance(dimensions, pd.DataFrame) and dimensions.empty):\n",
    "            logger.warning(\"No dimensions returned from API\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        logger.info(f\"Successfully fetched {len(dimensions)} dimensions\")\n",
    "        return dimensions\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"API method error - getDimensions may not be available: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch dimensions: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_dataview_info(cja: cjapy.CJA, data_view_id: str) -> dict:\n",
    "    \"\"\"Fetch data view information with error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Fetching data view information: {data_view_id}\")\n",
    "        lookup_data = cja.getDataView(data_view_id)\n",
    "        \n",
    "        if not lookup_data:\n",
    "            logger.error(\"Data view information returned empty\")\n",
    "            return {\"name\": \"Unknown\", \"id\": data_view_id}\n",
    "        \n",
    "        logger.info(f\"Successfully fetched data view info: {lookup_data.get('name', 'Unknown')}\")\n",
    "        return lookup_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch data view information: {str(e)}\")\n",
    "        return {\"name\": \"Unknown\", \"id\": data_view_id, \"error\": str(e)}\n",
    "\n",
    "# Fetch all data with error handling\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Starting data fetch operations\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "metrics = fetch_metrics(cja, data_view)\n",
    "dimensions = fetch_dimensions(cja, data_view)\n",
    "lookup_data = fetch_dataview_info(cja, data_view)\n",
    "\n",
    "# Check if we have any data to process\n",
    "if metrics.empty and dimensions.empty:\n",
    "    logger.critical(\"No metrics or dimensions fetched. Cannot generate SDR.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(\"Data fetch operations completed successfully\")\n",
    "\n",
    "# ==================== DATA QUALITY VALIDATION ====================\n",
    "\n",
    "class DataQualityChecker:\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.issues = []\n",
    "        self.logger = logger\n",
    "    \n",
    "    def add_issue(self, severity: str, category: str, item_type: str, \n",
    "                  item_name: str, description: str, details: str = \"\"):\n",
    "        \"\"\"Add a data quality issue to the tracker\"\"\"\n",
    "        self.issues.append({\n",
    "            'Severity': severity,\n",
    "            'Category': category,\n",
    "            'Type': item_type,\n",
    "            'Item Name': item_name,\n",
    "            'Issue': description,\n",
    "            'Details': details\n",
    "        })\n",
    "        self.logger.warning(f\"DQ Issue [{severity}] - {item_type}: {description}\")\n",
    "    \n",
    "    def check_duplicates(self, df: pd.DataFrame, item_type: str):\n",
    "        \"\"\"Check for duplicate names in metrics or dimensions\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.logger.info(f\"Skipping duplicate check for empty {item_type} dataframe\")\n",
    "                return\n",
    "            \n",
    "            if 'name' not in df.columns:\n",
    "                self.logger.warning(f\"'name' column not found in {item_type}. Skipping duplicate check.\")\n",
    "                return\n",
    "            \n",
    "            duplicates = df['name'].value_counts()\n",
    "            duplicates = duplicates[duplicates > 1]\n",
    "            \n",
    "            for name, count in duplicates.items():\n",
    "                self.add_issue(\n",
    "                    severity='HIGH',\n",
    "                    category='Duplicates',\n",
    "                    item_type=item_type,\n",
    "                    item_name=str(name),\n",
    "                    description=f'Duplicate name found {count} times',\n",
    "                    details=f'This {item_type.lower()} name appears {count} times in the data view'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking duplicates for {item_type}: {str(e)}\")\n",
    "    \n",
    "    def check_required_fields(self, df: pd.DataFrame, item_type: str, \n",
    "                            required_fields: List[str]):\n",
    "        \"\"\"Validate that required fields are present\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.logger.info(f\"Skipping required fields check for empty {item_type} dataframe\")\n",
    "                return\n",
    "            \n",
    "            missing_fields = [field for field in required_fields if field not in df.columns]\n",
    "            \n",
    "            if missing_fields:\n",
    "                self.add_issue(\n",
    "                    severity='CRITICAL',\n",
    "                    category='Missing Fields',\n",
    "                    item_type=item_type,\n",
    "                    item_name='N/A',\n",
    "                    description=f'Required fields missing from API response',\n",
    "                    details=f'Missing fields: {\", \".join(missing_fields)}'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking required fields for {item_type}: {str(e)}\")\n",
    "    \n",
    "    def check_null_values(self, df: pd.DataFrame, item_type: str, \n",
    "                         critical_fields: List[str]):\n",
    "        \"\"\"Check for null values in critical fields\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.logger.info(f\"Skipping null value check for empty {item_type} dataframe\")\n",
    "                return\n",
    "            \n",
    "            for field in critical_fields:\n",
    "                if field in df.columns:\n",
    "                    null_count = df[field].isna().sum()\n",
    "                    if null_count > 0:\n",
    "                        null_items = df[df[field].isna()]['name'].tolist() if 'name' in df.columns else []\n",
    "                        self.add_issue(\n",
    "                            severity='MEDIUM',\n",
    "                            category='Null Values',\n",
    "                            item_type=item_type,\n",
    "                            item_name=', '.join(str(x) for x in null_items[:5]),\n",
    "                            description=f'Null values in \"{field}\" field',\n",
    "                            details=f'{null_count} item(s) missing {field}. Items: {\", \".join(str(x) for x in null_items[:10])}'\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking null values for {item_type}: {str(e)}\")\n",
    "    \n",
    "    def check_missing_descriptions(self, df: pd.DataFrame, item_type: str):\n",
    "        \"\"\"Check for items without descriptions\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.logger.info(f\"Skipping description check for empty {item_type} dataframe\")\n",
    "                return\n",
    "            \n",
    "            if 'description' not in df.columns:\n",
    "                self.logger.info(f\"'description' column not found in {item_type}\")\n",
    "                return\n",
    "            \n",
    "            missing_desc = df[df['description'].isna() | (df['description'] == '')]\n",
    "            \n",
    "            if len(missing_desc) > 0:\n",
    "                item_names = missing_desc['name'].tolist() if 'name' in missing_desc.columns else []\n",
    "                self.add_issue(\n",
    "                    severity='LOW',\n",
    "                    category='Missing Descriptions',\n",
    "                    item_type=item_type,\n",
    "                    item_name=f'{len(missing_desc)} items',\n",
    "                    description=f'{len(missing_desc)} items without descriptions',\n",
    "                    details=f'Items: {\", \".join(str(x) for x in item_names[:20])}'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking descriptions for {item_type}: {str(e)}\")\n",
    "    \n",
    "    def check_empty_dataframe(self, df: pd.DataFrame, item_type: str):\n",
    "        \"\"\"Check if dataframe is empty\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.add_issue(\n",
    "                    severity='CRITICAL',\n",
    "                    category='Empty Data',\n",
    "                    item_type=item_type,\n",
    "                    item_name='N/A',\n",
    "                    description=f'No {item_type.lower()} found in data view',\n",
    "                    details=f'The API returned an empty dataset for {item_type.lower()}'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking if {item_type} dataframe is empty: {str(e)}\")\n",
    "    \n",
    "    def check_id_validity(self, df: pd.DataFrame, item_type: str):\n",
    "        \"\"\"Check for missing or invalid IDs\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                self.logger.info(f\"Skipping ID validity check for empty {item_type} dataframe\")\n",
    "                return\n",
    "            \n",
    "            if 'id' not in df.columns:\n",
    "                self.logger.warning(f\"'id' column not found in {item_type}\")\n",
    "                return\n",
    "            \n",
    "            missing_ids = df[df['id'].isna() | (df['id'] == '')]\n",
    "            if len(missing_ids) > 0:\n",
    "                self.add_issue(\n",
    "                    severity='HIGH',\n",
    "                    category='Invalid IDs',\n",
    "                    item_type=item_type,\n",
    "                    item_name=f'{len(missing_ids)} items',\n",
    "                    description=f'{len(missing_ids)} items with missing or invalid IDs',\n",
    "                    details='Items without valid IDs may cause issues in reporting'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking ID validity for {item_type}: {str(e)}\")\n",
    "    \n",
    "    def get_issues_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Return all issues as a DataFrame\"\"\"\n",
    "        try:\n",
    "            if not self.issues:\n",
    "                self.logger.info(\"No data quality issues found\")\n",
    "                return pd.DataFrame({\n",
    "                    'Severity': ['INFO'],\n",
    "                    'Category': ['Data Quality'],\n",
    "                    'Type': ['All'],\n",
    "                    'Item Name': ['N/A'],\n",
    "                    'Issue': ['No data quality issues detected'],\n",
    "                    'Details': ['All validation checks passed successfully']\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(self.issues).sort_values(\n",
    "                by=['Severity', 'Category'],\n",
    "                ascending=[False, True]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating issues dataframe: {str(e)}\")\n",
    "            return pd.DataFrame({\n",
    "                'Severity': ['ERROR'],\n",
    "                'Category': ['System'],\n",
    "                'Type': ['Processing'],\n",
    "                'Item Name': ['N/A'],\n",
    "                'Issue': ['Error generating data quality report'],\n",
    "                'Details': [str(e)]\n",
    "            })\n",
    "\n",
    "# Initialize data quality checker\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Starting data quality validation\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "dq_checker = DataQualityChecker(logger)\n",
    "\n",
    "# Required fields for validation\n",
    "REQUIRED_METRIC_FIELDS = ['id', 'name', 'type']\n",
    "REQUIRED_DIMENSION_FIELDS = ['id', 'name', 'type']\n",
    "CRITICAL_FIELDS = ['id', 'name', 'title', 'description']\n",
    "\n",
    "# Run all data quality checks\n",
    "logger.info(\"Running comprehensive data quality checks...\")\n",
    "\n",
    "try:\n",
    "    # Check if dataframes are empty\n",
    "    dq_checker.check_empty_dataframe(metrics, 'Metrics')\n",
    "    dq_checker.check_empty_dataframe(dimensions, 'Dimensions')\n",
    "    \n",
    "    # Check for required fields\n",
    "    dq_checker.check_required_fields(metrics, 'Metrics', REQUIRED_METRIC_FIELDS)\n",
    "    dq_checker.check_required_fields(dimensions, 'Dimensions', REQUIRED_DIMENSION_FIELDS)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dq_checker.check_duplicates(metrics, 'Metrics')\n",
    "    dq_checker.check_duplicates(dimensions, 'Dimensions')\n",
    "    \n",
    "    # Check for null values in critical fields\n",
    "    dq_checker.check_null_values(metrics, 'Metrics', CRITICAL_FIELDS)\n",
    "    dq_checker.check_null_values(dimensions, 'Dimensions', CRITICAL_FIELDS)\n",
    "    \n",
    "    # Check for missing descriptions\n",
    "    dq_checker.check_missing_descriptions(metrics, 'Metrics')\n",
    "    dq_checker.check_missing_descriptions(dimensions, 'Dimensions')\n",
    "    \n",
    "    # Check ID validity\n",
    "    dq_checker.check_id_validity(metrics, 'Metrics')\n",
    "    dq_checker.check_id_validity(dimensions, 'Dimensions')\n",
    "    \n",
    "    logger.info(f\"Data quality checks complete. Found {len(dq_checker.issues)} issue(s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data quality validation: {str(e)}\")\n",
    "    logger.info(\"Continuing with SDR generation despite validation errors\")\n",
    "\n",
    "# Get data quality issues dataframe\n",
    "data_quality_df = dq_checker.get_issues_dataframe()\n",
    "\n",
    "# ==================== DATA PROCESSING ====================\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Processing data for Excel export\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Process lookup data into DataFrame\n",
    "    logger.info(\"Processing data view lookup information...\")\n",
    "    lookup_data = {k: [v] if not isinstance(v, (list, tuple)) else v for k, v in lookup_data.items()}\n",
    "    max_length = max(len(v) for v in lookup_data.values()) if lookup_data else 1\n",
    "    lookup_data = {k: v + [None] * (max_length - len(v)) for k, v in lookup_data.items()}\n",
    "    lookup_df = pd.DataFrame(lookup_data)\n",
    "    logger.info(f\"Processed lookup data with {len(lookup_df)} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing lookup data: {str(e)}\")\n",
    "    lookup_df = pd.DataFrame({'Error': ['Failed to process data view information']})\n",
    "\n",
    "try:\n",
    "    # Enhanced metadata creation\n",
    "    logger.info(\"Creating metadata summary...\")\n",
    "    metric_types = metrics['type'].value_counts().to_dict() if not metrics.empty and 'type' in metrics.columns else {}\n",
    "    metric_summary = [f\"{type_}: {count}\" for type_, count in metric_types.items()]\n",
    "    \n",
    "    dimension_types = dimensions['type'].value_counts().to_dict() if not dimensions.empty and 'type' in dimensions.columns else {}\n",
    "    dimension_summary = [f\"{type_}: {count}\" for type_, count in dimension_types.items()]\n",
    "    \n",
    "    # Get current timezone and formatted timestamp\n",
    "    local_tz = datetime.now().astimezone().tzinfo\n",
    "    current_time = datetime.now(local_tz)\n",
    "    formatted_timestamp = current_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    \n",
    "    # Count data quality issues by severity\n",
    "    severity_counts = data_quality_df['Severity'].value_counts().to_dict()\n",
    "    dq_summary = [f\"{sev}: {count}\" for sev, count in severity_counts.items()]\n",
    "    \n",
    "    # Create enhanced metadata DataFrame\n",
    "    metadata_df = pd.DataFrame({\n",
    "        'Property': [\n",
    "            'Generated Date & timestamp and timezone',\n",
    "            'Data View ID',\n",
    "            'Data View Name',\n",
    "            'Total Metrics',\n",
    "            'Metrics Breakdown',\n",
    "            'Total Dimensions',\n",
    "            'Dimensions Breakdown',\n",
    "            'Data Quality Issues',\n",
    "            'Data Quality Summary'\n",
    "        ],\n",
    "        'Value': [\n",
    "            formatted_timestamp,\n",
    "            data_view,\n",
    "            lookup_data.get(\"name\", [\"Unknown\"])[0] if isinstance(lookup_data, dict) else \"Unknown\",\n",
    "            len(metrics),\n",
    "            '\\n'.join(metric_summary) if metric_summary else 'No metrics found',\n",
    "            len(dimensions),\n",
    "            '\\n'.join(dimension_summary) if dimension_summary else 'No dimensions found',\n",
    "            len(dq_checker.issues),\n",
    "            '\\n'.join(dq_summary) if dq_summary else 'No issues'\n",
    "        ]\n",
    "    })\n",
    "    logger.info(\"Metadata created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating metadata: {str(e)}\")\n",
    "    metadata_df = pd.DataFrame({'Error': ['Failed to create metadata']})\n",
    "\n",
    "# Function to format JSON cells\n",
    "def format_json_cell(value):\n",
    "    \"\"\"Format JSON objects for Excel display\"\"\"\n",
    "    try:\n",
    "        if isinstance(value, (dict, list)):\n",
    "            return json.dumps(value, indent=2)\n",
    "        return value\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error formatting JSON cell: {str(e)}\")\n",
    "        return str(value)\n",
    "\n",
    "try:\n",
    "    # Apply JSON formatting to all dataframes\n",
    "    logger.info(\"Applying JSON formatting to dataframes...\")\n",
    "    \n",
    "    for col in lookup_df.columns:\n",
    "        lookup_df[col] = lookup_df[col].map(format_json_cell)\n",
    "    \n",
    "    if not metrics.empty:\n",
    "        for col in metrics.columns:\n",
    "            metrics[col] = metrics[col].map(format_json_cell)\n",
    "    \n",
    "    if not dimensions.empty:\n",
    "        for col in dimensions.columns:\n",
    "            dimensions[col] = dimensions[col].map(format_json_cell)\n",
    "    \n",
    "    logger.info(\"JSON formatting applied successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error applying JSON formatting: {str(e)}\")\n",
    "\n",
    "# Create Excel file name\n",
    "try:\n",
    "    dv_name = lookup_data.get(\"name\", [\"Unknown\"])[0] if isinstance(lookup_data, dict) else \"Unknown\"\n",
    "    # Sanitize filename\n",
    "    dv_name = \"\".join(c for c in dv_name if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    excel_file_name = f'CJA_DataView_{dv_name}_{data_view}_SDR.xlsx'\n",
    "    logger.info(f\"Excel file will be saved as: {excel_file_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating filename: {str(e)}\")\n",
    "    excel_file_name = f'CJA_DataView_{data_view}_SDR.xlsx'\n",
    "\n",
    "# ==================== EXCEL GENERATION ====================\n",
    "\n",
    "def apply_excel_formatting(writer, df, sheet_name):\n",
    "    \"\"\"Apply formatting to Excel sheets with error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Formatting sheet: {sheet_name}\")\n",
    "        \n",
    "        # Write dataframe to sheet\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        \n",
    "        # Add formats\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True,\n",
    "            'bg_color': '#366092',\n",
    "            'font_color': 'white',\n",
    "            'border': 1,\n",
    "            'align': 'center',\n",
    "            'text_wrap': True\n",
    "        })\n",
    "        \n",
    "        grey_format = workbook.add_format({\n",
    "            'bg_color': '#F2F2F2',\n",
    "            'border': 1,\n",
    "            'text_wrap': True,\n",
    "            'align': 'top',\n",
    "            'valign': 'top'\n",
    "        })\n",
    "        \n",
    "        white_format = workbook.add_format({\n",
    "            'bg_color': '#FFFFFF',\n",
    "            'border': 1,\n",
    "            'text_wrap': True,\n",
    "            'align': 'top',\n",
    "            'valign': 'top'\n",
    "        })\n",
    "        \n",
    "        # Special formats for Data Quality sheet\n",
    "        if sheet_name == 'Data Quality':\n",
    "            critical_format = workbook.add_format({\n",
    "                'bg_color': '#FFC7CE',\n",
    "                'font_color': '#9C0006',\n",
    "                'border': 1,\n",
    "                'text_wrap': True,\n",
    "                'align': 'top',\n",
    "                'valign': 'top'\n",
    "            })\n",
    "            \n",
    "            high_format = workbook.add_format({\n",
    "                'bg_color': '#FFEB9C',\n",
    "                'font_color': '#9C6500',\n",
    "                'border': 1,\n",
    "                'text_wrap': True,\n",
    "                'align': 'top',\n",
    "                'valign': 'top'\n",
    "            })\n",
    "            \n",
    "            medium_format = workbook.add_format({\n",
    "                'bg_color': '#C6EFCE',\n",
    "                'font_color': '#006100',\n",
    "                'border': 1,\n",
    "                'text_wrap': True,\n",
    "                'align': 'top',\n",
    "                'valign': 'top'\n",
    "            })\n",
    "            \n",
    "            low_format = workbook.add_format({\n",
    "                'bg_color': '#DDEBF7',\n",
    "                'font_color': '#1F4E78',\n",
    "                'border': 1,\n",
    "                'text_wrap': True,\n",
    "                'align': 'top',\n",
    "                'valign': 'top'\n",
    "            })\n",
    "        \n",
    "        # Format header row\n",
    "        for col_num, value in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # Set row height and column width with text wrapping\n",
    "        max_column_width = 100\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            series = df[col]\n",
    "            max_len = min(\n",
    "                max(\n",
    "                    max(len(str(val).split('\\n')[0]) for val in series),\n",
    "                    len(str(series.name))\n",
    "                ) + 2,\n",
    "                max_column_width\n",
    "            )\n",
    "            worksheet.set_column(idx, idx, max_len)\n",
    "        \n",
    "        # Apply row formatting\n",
    "        for idx in range(len(df)):\n",
    "            max_lines = max(str(val).count('\\n') for val in df.iloc[idx]) + 1\n",
    "            row_height = min(max_lines * 15, 400)\n",
    "            \n",
    "            # Apply severity-based formatting for Data Quality sheet\n",
    "            if sheet_name == 'Data Quality' and 'Severity' in df.columns:\n",
    "                severity = df.iloc[idx]['Severity']\n",
    "                if severity == 'CRITICAL':\n",
    "                    row_format = critical_format\n",
    "                elif severity == 'HIGH':\n",
    "                    row_format = high_format\n",
    "                elif severity == 'MEDIUM':\n",
    "                    row_format = medium_format\n",
    "                else:\n",
    "                    row_format = low_format\n",
    "            else:\n",
    "                row_format = grey_format if idx % 2 == 0 else white_format\n",
    "            \n",
    "            worksheet.set_row(idx + 1, row_height, row_format)\n",
    "        \n",
    "        # Add autofilter to all sheets\n",
    "        worksheet.autofilter(0, 0, len(df), len(df.columns) - 1)\n",
    "        \n",
    "        # Freeze top row\n",
    "        worksheet.freeze_panes(1, 0)\n",
    "        \n",
    "        logger.info(f\"Successfully formatted sheet: {sheet_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error formatting sheet {sheet_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Write to Excel with formatting\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Generating Excel file\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Creating Excel writer for: {excel_file_name}\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file_name, engine='xlsxwriter') as writer:\n",
    "        # Write sheets in order, with Data Quality first for visibility\n",
    "        sheets_to_write = [\n",
    "            (metadata_df, 'Metadata'),\n",
    "            (data_quality_df, 'Data Quality'),\n",
    "            (lookup_df, 'DataView'),\n",
    "            (metrics, 'Metrics'),\n",
    "            (dimensions, 'Dimensions')\n",
    "        ]\n",
    "        \n",
    "        for sheet_data, sheet_name in sheets_to_write:\n",
    "            try:\n",
    "                if sheet_data.empty:\n",
    "                    logger.warning(f\"Sheet {sheet_name} is empty, creating placeholder\")\n",
    "                    placeholder_df = pd.DataFrame({'Note': [f'No data available for {sheet_name}']})\n",
    "                    apply_excel_formatting(writer, placeholder_df, sheet_name)\n",
    "                else:\n",
    "                    apply_excel_formatting(writer, sheet_data, sheet_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to write sheet {sheet_name}: {str(e)}\")\n",
    "                # Continue with other sheets\n",
    "                continue\n",
    "    \n",
    "    logger.info(f\"✓ SDR generation complete! File saved as: {excel_file_name}\")\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"EXECUTION SUMMARY\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"Data View: {dv_name} ({data_view})\")\n",
    "    logger.info(f\"Metrics: {len(metrics)}\")\n",
    "    logger.info(f\"Dimensions: {len(dimensions)}\")\n",
    "    logger.info(f\"Data Quality Issues: {len(dq_checker.issues)}\")\n",
    "    \n",
    "    if dq_checker.issues:\n",
    "        logger.info(\"Data Quality Issues by Severity:\")\n",
    "        for severity, count in severity_counts.items():\n",
    "            logger.info(f\"  {severity}: {count}\")\n",
    "    \n",
    "    logger.info(f\"Output file: {excel_file_name}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "except PermissionError as e:\n",
    "    logger.critical(f\"Permission denied writing to {excel_file_name}. File may be open in another program.\")\n",
    "    logger.critical(\"Please close the file and try again.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to generate Excel file: {str(e)}\")\n",
    "    logger.exception(\"Full exception details:\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(\"Script execution completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cja_auto_sdr_2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
