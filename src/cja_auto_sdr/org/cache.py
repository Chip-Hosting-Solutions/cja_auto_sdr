"""
Org-wide report caching functionality.

This module provides caching for data view component data to speed up
repeat analysis runs, and a lock mechanism to prevent concurrent runs.
"""

from __future__ import annotations

import contextlib
import errno
import json
import logging
import os
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from cja_auto_sdr.core.locks.manager import LockManager
from cja_auto_sdr.org.models import DataViewSummary


class OrgReportLock:
    """Cross-process lock to prevent concurrent org-report runs for one org.

    Ownership is determined by backend lock primitives (OS advisory lock
    by default). File metadata is diagnostic only and does not decide lock
    ownership.

    Usage:
        with OrgReportLock(org_id) as lock:
            if not lock.acquired:
                print("Another org-report is running")
                return
            # ... run analysis ...

    Args:
        org_id: Organization ID to lock
        lock_dir: Directory for lock files. Defaults to ~/.cja_auto_sdr/locks/
        stale_threshold_seconds: Lease staleness for fallback backend heartbeat/recovery.
        lock_backend: Optional backend override ("auto", "fcntl", "lease")
    """

    def __init__(
        self,
        org_id: str,
        lock_dir: Path | None = None,
        stale_threshold_seconds: int = 3600,
        lock_backend: str | None = None,
    ):
        if lock_dir is None:
            lock_dir = Path.home() / ".cja_auto_sdr" / "locks"
        self.lock_dir = lock_dir
        # Sanitize org_id for filename (keep only safe chars)
        import re

        safe_org_id = re.sub(r"[^a-zA-Z0-9_-]", "_", org_id)
        self.lock_file = lock_dir / f"org_report_{safe_org_id}.lock"
        self.stale_threshold = stale_threshold_seconds
        self.acquired = False
        self._manager = LockManager(
            lock_path=self.lock_file,
            owner=org_id,
            stale_threshold_seconds=stale_threshold_seconds,
            backend_name=lock_backend,
        )

    def __enter__(self) -> OrgReportLock:
        self.acquired = self._try_acquire()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        if self.acquired:
            self._release()

    def _try_acquire(self) -> bool:
        """Attempt to acquire lock non-blocking."""
        return self._manager.acquire()

    def _release(self) -> None:
        """Release lock if currently held by this process."""
        self._manager.release()

    @property
    def lock_lost(self) -> bool:
        return self._manager.lock_lost

    def ensure_healthy(self) -> None:
        """Raise if lock ownership has been lost during execution."""
        self._manager.ensure_held()

    @staticmethod
    def _is_process_running(pid: int) -> bool:
        """Legacy helper kept for compatibility with existing tests."""
        if isinstance(pid, bool):
            return False
        try:
            normalized_pid = int(pid)
        except TypeError, ValueError, OverflowError:
            return False
        if normalized_pid <= 0:
            return False
        try:
            os.kill(normalized_pid, 0)  # Signal 0 doesn't kill, just checks
            return True
        except ProcessLookupError:
            return False
        except PermissionError:
            # EPERM means the process exists but we do not have permission to signal it.
            return True
        except OverflowError:
            return False
        except OSError as e:
            return e.errno == errno.EPERM

    def get_lock_info(self) -> dict[str, Any] | None:
        """Get information about the current lock holder.

        Returns:
            Dict with pid, timestamp, started_at or None if no lock
        """
        return self._manager.read_info()


class OrgReportCache:
    """Cache for org-wide report data view components.

    Stores fetched DataViewSummary objects to disk for faster repeat runs.
    Cache is JSON-based and stores component IDs, counts, and metadata.
    """

    def __init__(self, cache_dir: Path | None = None, logger: logging.Logger | None = None):
        """Initialize the cache.

        Args:
            cache_dir: Directory for cache files. Defaults to ~/.cja_auto_sdr/cache/
            logger: Optional logger for cache load/save warnings
        """
        if cache_dir is None:
            cache_dir = Path.home() / ".cja_auto_sdr" / "cache"
        self.cache_dir = cache_dir
        self.cache_file = cache_dir / "org_report_cache.json"
        self.logger = logger or logging.getLogger(__name__)
        self._cache: dict[str, dict[str, Any]] = {}
        self._load_cache()

    def _load_cache(self) -> None:
        """Load cache from disk."""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, encoding="utf-8") as f:
                    self._cache = json.load(f)
            except (OSError, json.JSONDecodeError) as e:
                self.logger.warning(f"Failed to load org report cache from {self.cache_file}: {e}")
                self._cache = {}

    def _save_cache(self) -> None:
        """Save cache to disk via atomic write-then-rename."""
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        tmp_path = self.cache_file.with_name(f".{self.cache_file.name}.{uuid.uuid4().hex}.tmp")
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                json.dump(self._cache, f, indent=2, default=str)
            os.replace(tmp_path, self.cache_file)
        except OSError as e:
            self.logger.warning(f"Failed to save org report cache to {self.cache_file}: {e}")
            with contextlib.suppress(OSError):
                tmp_path.unlink()

    def get(
        self,
        dv_id: str,
        max_age_hours: int = 24,
        required_flags: dict[str, bool] | None = None,
        current_modified: str | None = None,
    ) -> DataViewSummary | None:
        """Get cached data view summary if fresh enough.

        Args:
            dv_id: Data view ID
            max_age_hours: Maximum cache age in hours
            required_flags: Optional flags indicating required cached fields
            current_modified: Current data view modification timestamp for validation.
                If provided and differs from cached timestamp, returns None.

        Returns:
            Cached DataViewSummary or None if not cached or stale
        """
        if dv_id not in self._cache:
            return None

        entry = self._cache[dv_id]
        fetched_at = entry.get("fetched_at")
        if not fetched_at:
            return None

        try:
            fetched_time = datetime.fromisoformat(fetched_at)
            if datetime.now() - fetched_time > timedelta(hours=max_age_hours):
                return None  # Cache is stale
        except ValueError, TypeError:
            return None

        # Validate modification timestamp if provided
        # NOTE: If current_modified is None (API didn't return timestamp),
        # we skip validation and treat cached entry as valid (optimistic approach).
        # This prevents unnecessary refetches when metadata is unavailable.
        if current_modified is not None:
            cached_modified = entry.get("modified")
            if cached_modified != current_modified:
                return None  # Data view has been modified since cached

        if required_flags:
            include_names = required_flags.get("include_names", False)
            include_metadata = required_flags.get("include_metadata", False)
            include_component_types = required_flags.get("include_component_types", False)

            if include_names and not entry.get("include_names", False):
                return None
            if include_metadata and not entry.get("include_metadata", False):
                return None
            if include_component_types and not entry.get("include_component_types", False):
                return None

        # Reconstruct DataViewSummary from cached data
        try:
            return DataViewSummary(
                data_view_id=entry.get("data_view_id", dv_id),
                data_view_name=entry.get("data_view_name", "Unknown"),
                metric_ids=set(entry.get("metric_ids", [])),
                dimension_ids=set(entry.get("dimension_ids", [])),
                metric_count=entry.get("metric_count", 0),
                dimension_count=entry.get("dimension_count", 0),
                status=entry.get("status", "active"),
                fetch_duration=entry.get("fetch_duration", 0.0),
                error=entry.get("error"),
                metric_names=entry.get("metric_names"),
                dimension_names=entry.get("dimension_names"),
                standard_metric_count=entry.get("standard_metric_count", 0),
                derived_metric_count=entry.get("derived_metric_count", 0),
                standard_dimension_count=entry.get("standard_dimension_count", 0),
                derived_dimension_count=entry.get("derived_dimension_count", 0),
                owner=entry.get("owner"),
                owner_id=entry.get("owner_id"),
                created=entry.get("created"),
                modified=entry.get("modified"),
                description=entry.get("description"),
                has_description=entry.get("has_description", False),
            )
        except Exception:
            return None

    def put(
        self,
        summary: DataViewSummary,
        include_names: bool = False,
        include_metadata: bool = False,
        include_component_types: bool = False,
    ) -> None:
        """Store a DataViewSummary in the cache.

        Args:
            summary: DataViewSummary to cache
            include_names: Whether names were included in the summary
            include_metadata: Whether metadata was included in the summary
            include_component_types: Whether component type counts were included
        """
        self._cache[summary.data_view_id] = self._build_entry(
            summary,
            include_names=include_names,
            include_metadata=include_metadata,
            include_component_types=include_component_types,
        )
        self._save_cache()

    def put_many(
        self,
        summaries: list[DataViewSummary],
        include_names: bool = False,
        include_metadata: bool = False,
        include_component_types: bool = False,
    ) -> None:
        """Store multiple DataViewSummary objects in the cache with a single disk write."""
        if not summaries:
            return
        for summary in summaries:
            self._cache[summary.data_view_id] = self._build_entry(
                summary,
                include_names=include_names,
                include_metadata=include_metadata,
                include_component_types=include_component_types,
            )
        self._save_cache()

    def _build_entry(
        self,
        summary: DataViewSummary,
        *,
        include_names: bool,
        include_metadata: bool,
        include_component_types: bool,
    ) -> dict[str, Any]:
        return {
            "data_view_id": summary.data_view_id,
            "data_view_name": summary.data_view_name,
            "metric_ids": list(summary.metric_ids),
            "dimension_ids": list(summary.dimension_ids),
            "metric_count": summary.metric_count,
            "dimension_count": summary.dimension_count,
            "status": summary.status,
            "fetch_duration": summary.fetch_duration,
            "error": summary.error,
            "metric_names": summary.metric_names,
            "dimension_names": summary.dimension_names,
            "standard_metric_count": summary.standard_metric_count,
            "derived_metric_count": summary.derived_metric_count,
            "standard_dimension_count": summary.standard_dimension_count,
            "derived_dimension_count": summary.derived_dimension_count,
            "owner": summary.owner,
            "owner_id": summary.owner_id,
            "created": summary.created,
            "modified": summary.modified,
            "description": summary.description,
            "has_description": summary.has_description,
            "include_names": include_names,
            "include_metadata": include_metadata,
            "include_component_types": include_component_types,
            "fetched_at": datetime.now().isoformat(),
        }

    def invalidate(self, dv_id: str | None = None) -> None:
        """Clear cache entries.

        Args:
            dv_id: Specific data view ID to clear, or None to clear all
        """
        if dv_id is None:
            self._cache = {}
        elif dv_id in self._cache:
            del self._cache[dv_id]
        self._save_cache()

    def has_valid_entry(self, dv_id: str, max_age_hours: int = 24) -> bool:
        """Check if a cache entry exists and is within age limit.

        Args:
            dv_id: Data view ID
            max_age_hours: Maximum cache age in hours

        Returns:
            True if entry exists and is fresh enough to potentially use
        """
        if dv_id not in self._cache:
            return False

        entry = self._cache[dv_id]
        fetched_at = entry.get("fetched_at")
        if not fetched_at:
            return False

        try:
            fetched_time = datetime.fromisoformat(fetched_at)
            if datetime.now() - fetched_time > timedelta(hours=max_age_hours):
                return False  # Cache is stale anyway
        except ValueError, TypeError:
            return False

        return True

    def get_cached_modified(self, dv_id: str) -> str | None:
        """Get the cached modification timestamp for a data view.

        Args:
            dv_id: Data view ID

        Returns:
            Cached modification timestamp or None if not cached
        """
        if dv_id not in self._cache:
            return None
        return self._cache[dv_id].get("modified")

    def get_stats(self) -> dict[str, Any]:
        """Get cache statistics.

        Returns:
            Dict with cache statistics
        """
        return {
            "entries": len(self._cache),
            "cache_file": str(self.cache_file),
            "cache_size_bytes": self.cache_file.stat().st_size if self.cache_file.exists() else 0,
        }
